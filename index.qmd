---
title: "Forecasting Lending Amount in Australia Economy Through Bayesian VAR Approach"
author: "Zheyuan Li"

execute:
  echo: false
  
bibliography: references.bib
---

> Abstract: This paper aims to forecast the Australian lending amounts using a Bayesian Var approach. Briefly introduce the research method and and purpose.

> **Keywords.** bsvars, forecasting, credit market, lending amount, shrinkage, housing market

```{r}
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
```

```{r,message=FALSE, warning=FALSE}
library(readabs)
library(readrba)
library(zoo)
library(tseries)
library(tidyr)
library(dplyr)
library(forecast)
library(ggplot2)
library(lubridate)
library(httr)
library(xts)
library(readxl)
library(gridExtra)
library(knitr)
library(mvtnorm)
library(kableExtra)
response_1 <- GET("https://www.rba.gov.au/statistics/tables/xls/f01hist.xlsx?v=2024-04-03-12-05-38")
temp_file_1 <- tempfile(fileext = ".xlsx")
writeBin(content(response_1, "raw"), temp_file_1)
data <- read_excel(temp_file_1,sheet = 1, skip = 1 )
data <- data[-(1:12), ]
origin_date <- as.Date("1899-12-30")
data[[1]] <- origin_date + as.numeric(data[[1]]) - 2
dates = data[1]
Cash_Rate_Target <- data$`Cash Rate Target`;
Cash_Rate_Target = as.numeric(Cash_Rate_Target)
plot_data <- data.frame(Date = dates, Cash_Rate_Target = Cash_Rate_Target)
Cash_Rate_Target = na.omit(Cash_Rate_Target)
plot_data <- na.omit(plot_data[c("Title", "Cash_Rate_Target")])
plot_data <- plot_data %>% 
  filter(Title >= as.Date("1991-01-01") & Title <= as.Date("2023-09-30"))
```

```{r,message=FALSE, warning=FALSE}
response_2 <- GET("https://www.abs.gov.au/statistics/labour/employment-and-unemployment/labour-force-australia-detailed/feb-2024/6291014a.xlsx")
temp_file_2 <- tempfile(fileext = ".xlsx")
writeBin(content(response_2, "raw"), temp_file_2)
data_2 <- read_excel(temp_file_2, sheet = 2)
data_2 <- data_2[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_2[[1]] <- origin_date + as.numeric(data_2[[1]]) - 2
dates = data_2[1]
Unemployment <- data_2$`Unemployed total ;  Persons ;`
Unemployment = as.numeric(Unemployment)
Unemployment <- log(Unemployment)
plot_data_2 <- data.frame(Date = dates, Unemployment = Unemployment)
plot_data_2 <- plot_data_2 %>% 
  filter(...1 >= as.Date("1991-01-01") & ...1  <= as.Date("2023-09-30"))
```

```{r,message=FALSE, warning=FALSE}
response_3 <- GET("https://www.rba.gov.au/statistics/tables/xls/d03hist.xlsx?v=2024-04-02-00-51-45")
temp_file_3 <- tempfile(fileext = ".xlsx")
writeBin(content(response_3, "raw"), temp_file_3)
data_3 <- read_excel(temp_file_3, sheet = 1,skip = 1)
data_3 <- data_3[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_3[[1]] <- origin_date + as.numeric(data_3[[1]]) - 2
dates = data_3[1]
M1 <- data_3$M1
M1 = as.numeric(M1)
M1 <- log(M1)
plot_data_3 <- data.frame(Date = dates, M1_Amount = M1)
plot_data_3 <- na.omit(plot_data_3[c("Title", "M1_Amount")])
M1 = na.omit(M1)
plot_data_3 <- plot_data_3 %>% 
  filter(Title >= as.Date("1991-01-01") & Title <= as.Date("2023-09-30"))
```


```{r,message=FALSE, warning=FALSE}
response_5 = GET("https://www.abs.gov.au/statistics/economy/price-indexes-and-inflation/consumer-price-index-australia/dec-quarter-2023/640101.xlsx")
temp_file_5 <- tempfile(fileext = ".xlsx")
writeBin(content(response_5, "raw"), temp_file_5)
data_5 <- read_excel(temp_file_5, sheet = 2)
data_5 <- data_5[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_5[[1]] <- origin_date + as.numeric(data_5[[1]]) - 2
dates = data_5[1]
CPI <- data_5$`Index Numbers ;  All groups CPI ;  Australia ;`
CPI = as.numeric(CPI)
CPI <- log(CPI)
plot_data_5 <- data.frame(Date = dates, CPI_Amount = CPI)
plot_data_5 <- plot_data_5 %>% 
  filter(...1 >= as.Date("1991-01-01") & ...1 <= as.Date("2023-12-31"))
plot_data_5$`...1` <- as.Date(plot_data_5$`...1`, "%m %Y-%d")
plot_data_5$...1 <- as.Date(cut(plot_data_5$...1, breaks = "quarter"), format = "%Y-%m-%d") - 1

full_seq <- seq(from = as.Date(sprintf("%s-01-01", min(format(plot_data_5$...1, "%Y")))), 
                to = as.Date(sprintf("%s-12-01", max(format(plot_data_5$...1, "%Y")))), 
                by = "month")

zoo_data <- zoo(plot_data_5$CPI_Amount, plot_data_5$...1)

zoo_full <- zoo(order.by = full_seq)
zoo_merged <- merge(zoo_data, zoo_full, all = TRUE)

zoo_interpolated <- na.approx(zoo_merged)
plot_data_monthly <- data.frame(Date = index(zoo_interpolated), CPI_Amount = coredata(zoo_interpolated))
plot_data_5 <- plot_data_monthly[!duplicated(format(plot_data_monthly$Date, "%Y-%m")), ]
plot_data_5 <- plot_data_5 %>% 
  filter(Date >= as.Date("1991-01-01") & Date <= as.Date("2023-12-31"))
```

```{r,message=FALSE, warning=FALSE}
response_6 = GET("https://www.abs.gov.au/statistics/economy/national-accounts/australian-national-accounts-national-income-expenditure-and-product/dec-2023/5206002_Expenditure_Volume_Measures.xlsx")
temp_file_6 <- tempfile(fileext = ".xlsx")
writeBin(content(response_6, "raw"), temp_file_6)
data_6 <- read_excel(temp_file_6, sheet = 2)
data_6 <- data_6[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_6[[1]] <- origin_date + as.numeric(data_6[[1]]) - 2
dates = data_6[1]
GDP <- data_6$`GROSS DOMESTIC PRODUCT ;...124`
GDP = as.numeric(GDP)
GDP <- log(GDP)
plot_data_6 <- data.frame(Date = dates, GDP_Amount = GDP)
plot_data_6 <- plot_data_6 %>% 
  filter(...1 >= as.Date("1991-01-01") & ...1 <= as.Date("2023-12-31"))
plot_data_6$`...1` <- as.Date(plot_data_6$`...1`, "%m %Y-%d")
plot_data_6$...1 <- as.Date(cut(plot_data_6$...1, breaks = "quarter"), format = "%Y-%m-%d") - 1

full_seq <- seq(from = as.Date(sprintf("%s-01-01", min(format(plot_data_6$...1, "%Y")))), 
                to = as.Date(sprintf("%s-12-01", max(format(plot_data_6$...1, "%Y")))), 
                by = "month")

zoo_data <- zoo(plot_data_6$GDP_Amount, plot_data_6$...1)

zoo_full <- zoo(order.by = full_seq)
zoo_merged <- merge(zoo_data, zoo_full, all = TRUE)

zoo_interpolated <- na.approx(zoo_merged)
plot_data_monthly <- data.frame(Date = index(zoo_interpolated), GDP_Amount = coredata(zoo_interpolated))
plot_data_6 <- plot_data_monthly[!duplicated(format(plot_data_monthly$Date, "%Y-%m")), ]
plot_data_6 <- plot_data_6 %>% 
  filter(Date >= as.Date("1991-01-01") & Date <= as.Date("2023-12-31"))
```

```{r,message=FALSE, warning=FALSE}
response_7 = GET("https://www.rba.gov.au/statistics/tables/xls/d05hist.xlsx?v=2024-04-02-14-20-19")
temp_file_7 <- tempfile(fileext = ".xlsx")
writeBin(content(response_7, "raw"), temp_file_7)
data_7 <- read_excel(temp_file_7, sheet = 1, skip = 1)
data_7 <- data_7[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_7[[1]] <- origin_date + as.numeric(data_7[[1]]) - 2
dates = data_7[1]
Houselend <- data_7$`Lending to persons; Housing; Owner-occupiers`
Houselend = as.numeric(Houselend)
Houselend <- log(Houselend)
plot_data_7 <- data.frame(Date = dates, Houselend_Amount = Houselend)
plot_data_7 <- plot_data_7 %>% 
  filter(Title >= as.Date("1991-01-01") & Title <= as.Date("2023-09-30"))
```

```{r,message=FALSE, warning=FALSE}
response_8 = GET("https://www.rba.gov.au/statistics/tables/xls/d05hist.xlsx?v=2024-04-02-14-20-19")
temp_file_8 <- tempfile(fileext = ".xlsx")
writeBin(content(response_8, "raw"), temp_file_8)
data_8 <- read_excel(temp_file_8, sheet = 1, skip = 1)
data_8 <- data_8[-(1:9), ]
origin_date <- as.Date("1899-12-30")
data_8[[1]] <- origin_date + as.numeric(data_8[[1]]) - 2
dates = data_8[1]
Businesslend <- data_8$`Lending to government; Public sector securities`
Businesslend = as.numeric(Businesslend)
Businesslend <- log(Businesslend)
plot_data_8 <- data.frame(Date = dates, Businesslend_Amount = Businesslend)
Businesslend = na.omit(Businesslend)
plot_data_8 <- plot_data_8 %>% 
  filter(Title >= as.Date("1991-01-01") & Title <= as.Date("2023-09-30"))
```


## Introduction

This project is focusing on the credit market in Australia. It intends to use Bayesian VAR approach in forecasting of lending amounts in housing and business sector.

**The Research Objective and Question:** The research focuses on predicting lending trends in Australia, with an emphasis on the amounts of lending. Therefore, the question I want to answer is: How can we accurately predict Australia's lending indicators, and what do these forecasts reveal through BVAR?

**Motivation of Research:** For banks and financial institutions, managing the interest rate associated with future fluctuations is crucial. Precise forecasts of lending amounts can aid these institutions in more effectively assessing and managing lending rates, which in turn helps in protecting their assets and ensuring financial stability.

## Data and Properties

This section shows the data collection process and their descriptive statistic including their series plotting and basic properties through unit-root test.

### Variables Selection

The selection of variables grounds in several macroeconomic indicators, derived from the official databases of the Australian Bureau of Statistics (ABS) and the Reserve Bank of Australia (RBA). The rationale for the choice of variables is informed by the work of scholars Marta Bańbura, Domenico Giannone, and Michele Lenza (2015) on European macroeconomic forecasting. This research takes similar variables in macroeconomic forecasting,including the credit market.

These indicators are closely linked to lending market. An increase in CPI indices signifies a decline in the purchasing power of money, necessitating a rise in loan rates to compensate for inflationary losses. Growth in GDP indicates economic expansion, leading to increased demand for capital, and consequently, higher lending volumes. Higher unemployment rates suggest an economic downturn, prompting banks to lower interest rates to stimulate the economy. Additionally, an abundance of monetary supply leads banks to reduce loan rates to attract more borrowers. Also, the cash target rate will influences the loan amount by affecting borrowing costs; when lowered, borrowing becomes cheaper, stimulating loan demand, The cash target rate impacts loan market volume by altering borrowing costs: lower rates stimulate loan demand, while higher rates may reduce it.

Therefore, in my project, these following specific variables are used in BVAR model:

-   $m1_t$: M1 aggregate from RBA Database

-   $cpi_t$: CPI aggregate from ABS Database

-   $cashrate_t$: Cash Rate Target aggregate from RBA Database

-   $gdp_t$: Expenditure on Gross Domestic Product (GDP) aggregate from ABS Database

-   $unemploy_t$: Unemplyemnt person aggregate from ABS Database

For the loan volumes to be forecasted, I have categorized them into housing loans and commercial loans, as they represent two distinct types of demand. The underlying logic influenced by macroeconomic factors might differ between them:

-   $houselend_t$: House lending volume aggregate from RBA

-   $businesslend_t$: Business lending volume aggregate from RBA

### Transformation and Properties of the Variables

The cash rate target, $cashrate_t$, is in percentages so it doesn't nedd to be transformed. All other variables are applied in the log-transformation and the result are below:

-   $m1_t=\log(m1_t)$

-   $cpi_t=\log(cpi_t)$

-   $gdp_t=\log(gdp_t)$

-   $unemploy_t=\log(unemploy_t)$

-   $houselend_t=\log(houselend_t)$

-   $businesslend_t=\log(businesslend_t)$

Their time series plots result are below:

```{r warning=FALSE}
#| echo: false
#| message: false
#| warning: false
#| label: fig-line-plot
#| fig-cap: "Time Series Plots for Variables"

p1 = ggplot(plot_data, aes(x = Title, y = Cash_Rate_Target)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'Cash_Rate')

p2 = ggplot(plot_data_2, aes(x = ...1, y = Unemployment)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'Unemploy')

p3 = ggplot(plot_data_3, aes(x = Title, y = M1_Amount)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'M1')

p5 = ggplot(plot_data_5, aes(x = Date, y = CPI_Amount)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'CPI')

p6 = ggplot(plot_data_6, aes(x = Date, y = GDP_Amount)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'GDP')

p7 = ggplot(plot_data_7, aes(x = Title, y = Houselend_Amount)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'House Lend')

p8 = ggplot(plot_data_8, aes(x = Title, y = Businesslend_Amount)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(x = 'Date', y = 'Business Lend')

grid.arrange(p1, p2, p3, p5, p6, p7, p8, ncol = 2)
```

Variables' stationarity is crucial for robustness of models and prediction stability, so we need to use Augmented Dickey-Fuller (ADF) test to make sure that the variables are stationary.

Using the testing in levels, we can see except the cash rate target, all other variables are non-stationary because their p-values are lower than 0.05. So we will perform the ADF test on first diiference to confirm whether the variables are stationary.

```{r, warning=FALSE}
#| echo: false
#| label: tbl-adf-level 
#| tbl-cap: ADF test results - Levels 
data_list <- list(
  Cash_Rate_Target = Cash_Rate_Target,
  Unemployment = Unemployment,
  M1 = M1,
  CPI = CPI,
  GDP = GDP,
  Houselend = Houselend,
  Businesslend = Businesslend
)

adf_results <- lapply(names(data_list), function(name) {
  adf_test_result <- adf.test(data_list[[name]], k = 5)
  data.frame(
    Test = name,
    Test_Statistic = adf_test_result$statistic,
    P_Value = adf_test_result$p.value,
    Lags = 5
  )
})

adf_results <- do.call(rbind, adf_results)
rownames(adf_results) <- NULL
kable(adf_results, "html", align = 'c') %>% 
kable_styling(bootstrap_options = c("striped", "hover"))

```

In the table below. we can see it is clear that in ADF test in first difference, all variables are statistically significant in stationary.

```{r,warning=FALSE}
#| echo: false
#| label: tbl-adf-firstdifference  
#| tbl-cap: ADF test results - First Differences  
adf_diff_results <- lapply(names(data_list), function(name) {
  diff_series <- diff(data_list[[name]])
  adf_test_result <- adf.test(diff_series, k = 5)
  data.frame(
    Test = paste(name, "First Difference", sep = " - "),
    Test_Statistic = adf_test_result$statistic,
    P_Value = adf_test_result$p.value,
    Lags = 5
  )
})

adf_diff_results <- do.call(rbind, adf_diff_results)
rownames(adf_diff_results) <- NULL
kable(adf_diff_results, "html", align = 'c') %>% 
kable_styling(bootstrap_options = c("striped", "hover"))

```

The unit root test's result can also showed from the Autocorrelation Functions (ACF) and Partial Autocorrelation Functions (PACF) plots.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf-plot
#| fig-cap: "ACF Plots"
par(mfrow=c(2, 4), mar=c(5, 2, 4, 2) + 0.1, mgp=c(3, 1, 0), cex.main=1, cex.axis=1, cex.lab=1)

for(name in names(data_list)) {
  ts_data <- data_list[[name]]
  Acf(ts_data, main = name)
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf-plot
#| fig-cap: "PACF Plots"
par(mfrow=c(2, 4), mar=c(5, 2, 4, 2) + 0.1, mgp=c(3, 1, 0), cex.main=1, cex.axis=1, cex.lab=1)

for(name in names(data_list)) {
  ts_data <- data_list[[name]]
  Pacf(ts_data, main = name)
}

```

## Econometric Model and Hypothesis

To forecast the loan amount in housing and business sector, the Bayesian VAR model will be used in this project. Based on the credit forecast in previous research (Louzis, 2014), we simplify this project's notation of BVAR with $p$ lags below:

```{=tex}
\begin{align}
   y_t &= \mu_0 + A_1y_{t-1}+\dots+A_py_{t-p}+u_t
\end{align}
```
In this model, $y_t=[m1_t$ , $cpi_t$ , $cashrate_t$ , $gdp_t$ , $unemploy_t$ , $houselend_t$ , $businesslend_t]'$ and it includes the eight variables we list above. The variables we used in forecasting is $houselend_t$ and $businesslend_t$. The $u_t$ means the error term, which is assumed to be $u_t|Y_{t-1}\sim\;iidN(\textbf{0}_N,I_N)$, and in this following model $N=8$. $A_l$ with l = 1, 2,...,p is an $N\times N$ coefficient matrix for each lag. We can re-write it into matrix form:

```{=tex}
\begin{align}
  Y &= XA + U \\
  U &\sim MN(0, \Sigma \otimes I_T)
\end{align}
```

where $Y$ is a $T\times N$ matrix with $Y_{T \times N} = \left( y_1', y_2', ..., y_T' \right)'$. $X$ is a $T\times k$ matrix, where $k = 1 + Np$ as $p$ is the lag order. It can be written as $X_{T \times k} = \left( x_1', x_2', ..., x_T' \right)'$ and each $x_t' = (1, y_{t-1}',y_{t-2}',\dots, y_{t-p}')$ which is a $1 \times k$ matrix.$A$ is the $k\times N$ matrix which can be written as $A_{k \times N} = \left( a_0, A_1,A_2 ..., A_p \right)'$. $U_{T\times N} = \left( u_1', u_2', ..., u_T' \right)$ and it is a $T \times N$ matrix.

Through this model, I can make prediction by using information from historical macroeconomic data to estimate the probability distribution of future variables. The outcomes of these estimations will facilitate the interpretation of future loan market trends and the impact of various macroeconomic factors on the loan market.



## Estimation Procedure
### Basic Model 
```{r}
#Prepare data for BVAR#
start_date <- as.Date("1990-01-01")
end_date <- as.Date("2022-12-31")

date <- seq(from=as.Date("1991-01-01"), to=as.Date("2023-09-30"), by="month")

plot_data = data.frame(date = date, Cash_Rate_Target = plot_data$Cash_Rate_Target)
cash_rate = xts::xts(plot_data$Cash_Rate_Target, plot_data$date)

plot_data_2 = data.frame(date = date, Unemployment = plot_data_2$Unemployment)
unemploy = xts::xts(plot_data_2$Unemployment, plot_data_2$date)

plot_data_3 = data.frame(date = date, M1_Amount = plot_data_3$M1_Amount)
m1 = xts::xts(plot_data_3$M1_Amount, plot_data_3$date)

plot_data_5 = data.frame(date = date, CPI_Amount = plot_data_5$CPI_Amount)
cpi = xts::xts(plot_data_5$CPI_Amount, plot_data_5$date)

plot_data_6 = data.frame(date = date, GDP_Amount = plot_data_6$GDP_Amount)
gdp = xts::xts(plot_data_6$GDP_Amount, plot_data_6$date)

plot_data_7 = data.frame(date = date, Houselend_Amount = plot_data_7$Houselend_Amount)
houselend = xts::xts(plot_data_7$Houselend_Amount, plot_data_7$date)

plot_data_8 = data.frame(date = date, Businesslend_Amount = plot_data_8$Businesslend_Amount)
businesslend = xts::xts(plot_data_8$Businesslend_Amount, plot_data_8$date)

all_data = merge(houselend,businesslend,cash_rate,unemploy,m1,cpi,gdp)


```

The basic model for forecasting is built on the Normal Inverse Wishart distribution. The prior distribution of A, $\Sigma$ are shown below:

#### Prior distribution 

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma)\\
A|\Sigma &\sim MN_{K\times N}(\underline{A}, \Sigma,\underline{V} ) \\ 
\Sigma &\sim IW_{N}(\underline{S}, \underline{v})
\end{align}
```

In this formula:

- $\Sigma$ is a $K \times N$ square symmetric and positive definite matrix.

- $\underline{A}$ is a $K \times N$ matrix for the first lag of $houselend_t$.

- $\underline{V}$ means the shrinking level for $\underline{A}$. When $\underline{V}$ becomes higher, the shrinkage will be looser. 

- $\underline{S}$ is a $N$ vector diagonal matrix.

- $\underline{v}$ equals to $N+1$.


#### posterior distribution 

Based on the prior distribution, we can infer the posterior distribution below:

```{=tex}
\begin{align}
p(A,\Sigma | Y,X) &= p(A|Y,X,\Sigma)p(\Sigma|Y,X)\\
p(A|Y,X,\Sigma)&\sim MN_{K\times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X)&\sim IW_{N}(\overline{S}, \overline{v})
\end{align}
```

The parameters $overline{V}$, $overline{S}$, $overline{A}$, and $overline{v}$ are list below:

```{=tex}
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\overline{S} &= \underline{S}+Y'Y+\underline{A'}\underline{V}^{-1}\underline{A}-\overline{A'}\overline{V}^{-1}\overline{A} \\
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1}\underline{A}) \\
\overline{v} &= T + \underline{v} \\ 
```


Then we can drive the joint posterior distribution:

```{r static data setup}
#| echo: false
#| message: false
#| warning: false
y = ts(all_data[,1:ncol(all_data)])
Y = ts(y[13:nrow(y),], frequency=12)
X = matrix(1,nrow(Y),1)

for (i in 1:frequency(Y)){
  X  = cbind(X,y[13:nrow(y)-i,])
}
 
## Pre-setup 
N  = ncol(Y)
p  = frequency(Y)
A.hat = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Drive the Minnesota prior 
kappa.1       = 10              # shrinkage for A1 to Ap
kappa.2       = 200             # shrinkage for constant 
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
S.prior       = diag(diag(Sigma.hat))
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
nu.prior      = N+1
```


```{r}
#| echo: true
#| message: false
#| warning: false
## Posterior sample draw function for normal-inverse Wishard posterior parameters
posterior.draws  = function (S, Y, X){
    V.bar.inv = t(X)%*%X + diag(1/diag(V.prior))
    V.bar = solve(V.bar.inv)
    A.bar = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar = nrow(Y) + nu.prior
    S.bar  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
  
    # draw the posterior
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
    output  = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}


## Applying function 
# A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)


```

The graphs below show the posterior distribution for $houselend_t$ and $businesslend_t$:

```{r}
# Presenting output
head(round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6))         # posterior draw A
head(round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6))     

# posterior draw sigma

par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(posterior.sample.draws$A.posterior[2,1,], xlab = "Simulation times S", ylab = "houselend", col = "#05386B")    
hist(posterior.sample.draws$A.posterior[2,1,], xlab = "houselend", col = "#05386B", main = '')
plot.ts(posterior.sample.draws$Sigma.posterior[1,1,], xlab = "Simulation times S", ylab = "houselend sigma", col = "#5CDB95") 
hist(posterior.sample.draws$Sigma.posterior[1,1,], xlab = "houselend sigma", col = "#5CDB95", main = '')
plot.ts(posterior.sample.draws$A.posterior[3,2,], xlab = "Simulation times S", ylab = "businesslend", col = "#EDF5E1") 
hist(posterior.sample.draws$A.posterior[3,2,], xlab = "businesslend", col = "#EDF5E1", main = '')
plot.ts(posterior.sample.draws$Sigma.posterior[2,2,], xlab = "Simulation times S", ylab = "businesslend sigma", col = "#8EE4AF")   
hist(posterior.sample.draws$Sigma.posterior[2,2,], xlab = "businesslend sigma", col = "#8EE4AF", main = '')     

```



### Extended Model 
Compared to the basic model, the extended model contains a hierarchical structure and introduces a shrinkage parameter kappa $k$ that follows an inverse gamma 2 distribution. This new structure in the extended model will facilitate the modeling of the complexity inherent in time series.

The extended model's distribution is specified below:
```{=tex}
\begin{align}
p(A,\Sigma, k |Y,X) &\propto L(Y,X|A,\Sigma)p(A,\Sigma, k)\\
&\propto L(Y,X|A,\Sigma)p(A |\Sigma, k)p(\Sigma)p(k)
\end{align}
```


```{=tex}
\begin{align}
p(A |\Sigma, k) &\sim MN_{K\times N}(\underline{A}, \Sigma, k\underline{V})\\
p(\Sigma) &\sim IW_{N}(\underline{S},\underline{v})\\
p(k) &\sim IG2(\underline{S_{k}}, \underline{v_{k}} ) \\ 
\end{align}
```

Their full conditional posterior distribution of $k$ is:

```{=tex}
\begin{align}
p(k |Y,X,A,\Sigma ) &\propto L(Y,X|A,\Sigma)p(A,\Sigma, k)\\
&\propto L(Y,X|A,\Sigma)p(A |\Sigma, k)p(\Sigma)p(k) \\
&\propto p(A |\Sigma, k)p(k) 
\end{align}
```
```{=tex}
\begin{align}
p(k |Y,X,A,\Sigma ) &\propto \det(k\underline{V})^{-\frac{N}{2}}\exp\left\{-\frac{1}{2}TR[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})] \right\} k^{-\frac{\underline{v_{k}+2}}{2}}\exp\left\{ -\frac{1}{2}\frac{\underline{S_{k}}}{k} \right\} \\
& = k^{-\frac{kN+\underline{v_{k}+2}}{2}}\exp\left\{-\frac{1}{2}\frac{TR[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})]+\underline{S_{k}}}{k} \right\}
\end{align}
```

Each parameters' posterior distribution can be written below:

```{=tex}
\begin{align}
p(A |Y,X,\Sigma, k) &\sim MN_{K\times N}(\overline{A}, \Sigma, \overline{V})\\
p(\Sigma|Y,X,A,k) &\sim IW_{N}(\overline{S},\overline{v})\\
p(k |Y,X, A,\Sigma) &\sim IG2(\overline{S_{k}}, \overline{v_{k}} ) \\
\end{align}
```

These are the posterior distribution of $A$, $\Sigma$ and $k$.
The matrix $A$ follows a Matrix Normal distribution with mean matrix $\overline{A}$, covariance matrix $\Sigma$ and scaling matrix $\overline{V}$.

The covariance matrix $\Sigma$ follows Inverse Wishart distribution with a scale matrix $\overline{S}$ and degrees of freedom $\overline{v}$

The shrinkage parameter $k$ follows inverse Gamma 2 distribution with shape parameter $\overline{S_{k}}$ and scale parameter $\overline{v_{k}}$

And below are the specified values:

```{=tex}
\overline{V} &= (X'X + (k\underline{V})^{-1})^{-1}\\
\overline{A} &= \overline{V}(X'Y+(k\underline{V})^{-1}\underline{A}) \\
\overline{v} &= T+\underline{v}\\
\overline{S} &= \underline{S}+Y'Y+\underline{A}'(k\underline{V})^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A} \\
\overline{v_{k}} &= kN + \underline{v_{k}}\\
\overline{S_{k}} &= TR[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})]+\underline{S_{k}}\\

\end{align}
```


After we get the posterior distribution $p(A,\Sigma, k |Y,X)$, the Gibbs sampler method will be used for generating samples. This approach can deal with complex Bayesian model where can not find analytical solutions.

The graphs below show the posterior distribution for $houselend_t$ and $businesslend_t$ in extended model:


```{r function based on extended model}
#| echo: true
#| message: false
#| warning: false
# setup 
S1                = 5000                             # determine the burn-in draws
S2                = 20000                            # number of draws from the final simulation
total_step           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
k.posterior       = matrix(NA, S1+S2, 1)

k.posterior[1]    = 10                               # set k0 

# Prior IG2 distribution: kappa
S.k.prior         = 3
nu.k.prior        = 5

## Posterior sample draw function for extended model  
posterior.draws.exten = function (total_steptep, Y, X){
for (s in 1:total_step){
    # normal-inverse Wishard posterior parameters
    V.bar.inv              = t(X)%*%X + diag(1/ diag( k.posterior[s]* V.prior))
    V.bar                  = solve(V.bar.inv)
    A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag( k.posterior[s]* V.prior))%*%A.prior)
    nu.bar                 = nrow(Y) + nu.prior
    S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(k.posterior[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv              = solve(S.bar)
  
    # posterior draws for A and Sigma
    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
    Sigma.posterior[,,s]   = Sigma.posterior.draw
    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    # posterior draws for k
    if (s!=total_step){
    S.k.bar                = sum(diag( Sigma.posterior.IW[,,1] * t(A.posterior[,,s]-A.prior)%*%diag(1/diag(V.prior))%*%(A.posterior[,,s]-A.prior) )) + S.k.prior
    nu.k.bar               = (1+p*N)*N+ nu.k.prior 
    k.draw.tmp             = rchisq(1, df=nu.k.bar)
    k.draw                 = S.k.bar/k.draw.tmp
    k.posterior[s+1]       = k.draw
  }
}
    output                 = list (A.posterior.exten = A.posterior, Sigma.posterior.exten = Sigma.posterior, k.posterior.exten = k.posterior)
    return(output)
}
## Applying function 
posterior.ext = posterior.draws.exten(total_step = total_step, Y=Y, X=X)

```

```{r}

# Presenting output
head(round(apply(posterior.ext$A.posterior.exten[,,(S1+1):total_step], 1:2, mean),6))
head(round(apply(posterior.ext$Sigma.posterior.exten[,,(S1+1):total_step], 1:2, mean),6))
round(mean(posterior.ext$k.posterior.exten[(S1+1):total_step]),6)

par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(posterior.ext$A.posterior.exten[2,1,(S1+1):total_step], xlab = "Simulation times S", ylab = "houselend", col = "darkblue")   
hist(posterior.ext$A.posterior.exten[2,1,(S1+1):total_step], xlab = "houselend", col = "darkblue", main = '')
plot.ts(posterior.ext$Sigma.posterior.exten[1,1,(S1+1):total_step], xlab = "Simulation times S", ylab = "houselend_sigma", col = "lightblue") 
hist(posterior.ext$Sigma.posterior.exten[1,1,(S1+1):total_step], xlab = "houselend_sigma", col = "lightblue", main = '')
plot.ts(posterior.ext$A.posterior.exten[3,2,(S1+1):total_step], xlab = "Simulation times S", ylab = "businesslend", col = "darkred") 
hist(posterior.ext$A.posterior.exten[3,2,(S1+1):total_step], xlab = "businesslend", col = "darkred", main = '')
plot.ts(posterior.ext$Sigma.posterior.exten[2,2,(S1+1):total_step], xlab = "Simulation times S", ylab = "businesslend_sigma", col = "pink")   
hist(posterior.ext$Sigma.posterior.exten[2,2,(S1+1):total_step], xlab = "businesslend_sigma", col = "pink", main = '')

```
We can see that compared to basic model, the extended model gives more variability of $A$. For house lending amount, it has larger range which range from 0.9 to 1.3, and that in basic model range from 0.94 to 1.06. For business lending amount, the result from both models don't have strong difference.



### Proving Model 

In this section, I simulate 10000 observations for Gaussian random walk process to prove the models. The codes below can prove that the basic and extended models can replicate the correct variables in DGP.


```{r basic model prove}
#| echo: true
#| message: false
#| warning: false
#| eval: false
sim1 = cumsum(rnorm(10000, 0, sd=1))
sim2 = cumsum(rnorm(10000, 0, sd=1))
sim= cbind(sim1,sim2)

## Define data X, Y 
Y = ts(sim[2:nrow(sim),], frequency=1)
X = matrix(1,nrow(Y),1)
X = cbind(X,sim[2:nrow(sim)-1,])

## Test on basic model
N           = ncol(Y)
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1     = 1                                   # shrinkage for A1 to Ap
kappa.2     = 10                                  # shrinkage for constant 
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

# Applying function 
posterior.sample.draws = posterior.draws(S=100000, Y=Y, X=X)
round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6)
round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6)
```


```{r extended model prove}
#| echo: true
#| message: false
#| warning: false
#| eval: false

kappa.1     = 1                                
kappa.2     = 10                            
sim1        = 1000                              
sim2        = 10000                              
total     = sim1+sim2
A.posterior       = array(NA, dim = c((1+N*p),N,sim1+sim2))
Sigma.posterior   = array(NA, dim = c(N,N,sim1+sim2))
k.posterior       = matrix(NA, sim1+sim2, 1)
k.posterior[1]    = 10                         # set k0 

# Prior IG2 distribution: kappa
S.k.prior   = 2
nu.k.prior  = 4

# Applying function 
posterior.ext = posterior.draws.exten(total = total, Y=Y, X=X)
round(apply(posterior.ext$A.posterior.exten[,,(sim1+1):sim2], 1:2, mean),6)
round(apply(posterior.ext$Sigma.posterior.exten[,,(sim1+1):sim2], 1:2, mean),6)
```


## Model Forecasting 
In this section, the forecasting will focus on predicting the houselend amount and businesslend amount in next 12 months based on current data (until 2023.9). The forecasting will used the basic model and extended model together.

### Forecasting for basic BVAR model
```{r forecasting static data}
#| echo: false
#| message: false
#| warning: false
## Present data X, Y
y             = ts(all_data[,1:ncol(all_data)])
Y             = ts(y[13:nrow(y),], frequency=12)
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[13:nrow(y)-i,])
}
 
## Pre-setup 
N             = ncol(Y)
p             = frequency(Y)
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution specification - Minnesota prior 
kappa.1       = 10                                    # shrinkage for A1 to Ap
kappa.2       = 100                                   # shrinkage for constant 
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2,1]  = 1
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+1
```


```{r}

## Applying function 
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior

## 12-month ahead forecasting h=12
# set up
h                      = 12
S                      = 50000
Y.h                    = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T                = c(1,as.vector(t(x.Ti)))
    Y.f                = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
      x.Ti             = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]          = Y.f[1:N]
  }
}



```

Print the forecasting result for next 12 months for the house lending amount and business lending amount. 

```{r}
# transform houselend into annual houselend, and plot
par(mfrow=c(1,1), mar=c(6,6,3,3))
point.houselend.h            = ts(round(apply(Y.h[,1,],1,mean),6))
houselend.data.h             = c(y[,1], point.houselend.h)
houselend.data.h             = rev(houselend.data.h)
forecast.houselend             = matrix(NA, length(houselend.data.h)-3 , 1)
for (i in 1: (length(houselend.data.h))) {
     forecast.houselend[i]     = sum (houselend.data.h[i: (i+3)])
}
forecast.houselend            = ts(rev(forecast.houselend))
forecast.houselend             = xts(forecast.houselend, seq(as.Date("1991-01-01"), by = "month", length.out = length(forecast.houselend)))

plot.ts(forecast.houselend, lwd=2, col="black", xaxt='n')
axis(1,c(0,108,228,348,nrow(y)+h),c("1991","2000","2010","2020",""))
abline(h=0.02, col="red", lty = 2)
abline(v=393, col = "#8EE4AF", lwd=1.5)
abline(v=405, col = "#379683", lwd=1.5)
legend(98, 0.075, legend=c("Y2021", "Y2022"), col=c("#8EE4AF", "#379683"), lty=1, cex=0.6)
```


```{r}
# transform businesslend into annual businesslend, and plot
par(mfrow=c(1,1), mar=c(6,6,3,3))
point.businesslend.h            = ts(round(apply(Y.h[,2,],1,mean),6))
businesslend.data.h             = c(y[,2], point.businesslend.h)
businesslend.data.h             = rev(businesslend.data.h)
forecast.businesslend             = matrix(NA, length(businesslend.data.h)-3 , 1)
for (i in 1: (length(houselend.data.h))) {
     forecast.businesslend[i]     = sum (businesslend.data.h[i: (i+3)])
}
forecast.businesslend            = ts(rev(forecast.businesslend))
forecast.businesslend             = xts(forecast.businesslend, seq(as.Date("1991-01-01"), by = "month", length.out = length(forecast.businesslend)))

plot.ts(forecast.businesslend, lwd=2, col="black", xaxt='n')
axis(1,c(0,108,228,348,nrow(y)+h),c("1991","2000","2010","2020",""))
abline(h=0.02, col="red", lty = 2)
abline(v=393, col = "#8EE4AF", lwd=1.5)
abline(v=405, col = "#379683", lwd=1.5)
legend(98, 0.075, legend=c("Y2021", "Y2022"), col=c("#8EE4AF", "#379683"), lty=1, cex=0.6)
```




### Forecasting for extended BVAR model




## Reference

Louzis, Dimitrios P., Macroeconomic and Credit Forecasts in a Small Economy During Crisis: A Large Bayesian VAR Approach (June 1, 2014). Bank of Greece Working Paper No. 184, Available at SSRN: https://ssrn.com/abstract=4184651 or http://dx.doi.org/10.2139/ssrn.4184651

Bańbura, M., Giannone, D., & Lenza, M. (2015). Conditional forecasts and scenario analysis with vector autoregressions for large cross-sections. International Journal of Forecasting, 31(3), 739-756. https://doi.org/10.1016/j.ijforecast.2014.08.013.
